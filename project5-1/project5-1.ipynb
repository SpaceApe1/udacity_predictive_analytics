{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "5cba3963c37d400916a9e80ad5acd0c56daed180f4961ad2a064f19bb9bbda97"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# A/B Test: A New Menu Launch\n",
    "## Project Overview\n",
    "You're a business analyst for Round Roasters, a coffee restaurant in the United States of America. The executive team conducted a market test with a new menu and needs to figure whether the new menu can drive enough sales to offset the cost of marketing the new menu. Your job is to analyze the A/B test and write up a recommendation to whether the Round Roasters chain should launch this new menu."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load package\r\n",
    "from datetime import datetime, timedelta\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from scipy.stats import ttest_ind, ttest_rel\r\n",
    "from sklearn.neighbors import KDTree\r\n",
    "from sklearn.compose import ColumnTransformer\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "\r\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "# plt.style.use('seaborn-whitegrid')\r\n",
    "plt.rcParams['figure.figsize'] = [12, 12]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Plan Your Analysis\n",
    "To perform the correct analysis, you will need to prepare a data set. Prior to rolling up your sleeves and preparing the data, it’s a good idea to have a plan of what you need to do in order to prepare the correct data set. A good plan will help you with your analysis. Here are a few questions to get you started:\n",
    "\n",
    "-What is the performance metric you’ll use to evaluate the results of your test?  \n",
    "-What is the test period?  \n",
    "-At what level (day, week, month, etc.) should the data be aggregated?  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load Stores data\r\n",
    "stores_data = pd.read_csv('round-roaster-stores.csv')\r\n",
    "stores_data.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stores_data.head(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load Transactions data\r\n",
    "# force Invoice Date column to datetime \r\n",
    "transactions_data = pd.read_csv('RoundRoastersTransactions.csv', parse_dates=['Invoice Date'])\r\n",
    "transactions_data.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "transactions_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load Treatment Stores data\r\n",
    "treatment_stores_data = pd.read_csv('treatment-stores.csv')\r\n",
    "treatment_stores_data.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "treatment_stores_data.head(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Clean Up Your Data\n",
    "In this step, you should prepare the data for steps 3 and 4. You should aggregate the transaction data to the appropriate level and filter on the appropriate data ranges. You can assume that there is no missing, incomplete, duplicate, or dirty data. You’re ready to move on to the next step when you have weekly transaction data for all stores."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Test cities: Denver and Chicago\r\n",
    "# Treatment: 12 Weeks [2016-April-29 to 2016-July-21], start on Friday\r\n",
    "# Control: 12 Weeks [(2015-April-29 to 2015-July-21], start on Wednesday\r\n",
    "# Total weeks to identify trend and season: 52 Weeks + treatment weeks + control weeks = 52 + 12 + 12 = 76 Weeks\r\n",
    "\r\n",
    "data_end_date = datetime(2016, 7, 21)\r\n",
    "data_start_date = data_end_date - timedelta(weeks=76)\r\n",
    "print(f'Data Start Date: {data_start_date} \\nData End Date: {data_end_date}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Filter data for further process\r\n",
    "filtered_transactions_data = transactions_data.query('`Invoice Date` > @data_start_date and `Invoice Date` <= @data_end_date')\r\n",
    "# Make Invoice Date Index\r\n",
    "filtered_transactions_data.set_index('Invoice Date', inplace=True)\r\n",
    "\r\n",
    "# Aggregate the data to get the weekly gross margin and weekly traffic count unique invoices\r\n",
    "weekly_gross_and_traffic = filtered_transactions_data.groupby([pd.Grouper(freq='W-FRI', closed='left'), 'StoreID'], as_index=True).agg({'Gross Margin': 'sum', 'Invoice Number': 'nunique'}).reset_index()\r\n",
    "weekly_gross_and_traffic.rename(columns={'Invoice Number': 'Weekly Foot Traffic'}, inplace=True)\r\n",
    "\r\n",
    "# Hack to start week on first date\r\n",
    "weekly_gross_and_traffic['Invoice Date'] = weekly_gross_and_traffic['Invoice Date'] - pd.offsets.Week(1)\r\n",
    "\r\n",
    "# Create Trend and Seasonal\r\n",
    "result = seasonal_decompose(weekly_gross_and_traffic['Weekly Foot Traffic'], period=12, extrapolate_trend='freq')\r\n",
    "\r\n",
    "# Add Trend and Seasonal weekly gross data\r\n",
    "weekly_gross_and_traffic = weekly_gross_and_traffic.assign(Trend = result.trend, Seasonal = result.seasonal)\r\n",
    "# Test the progress\r\n",
    "weekly_gross_and_traffic.query('StoreID == 10018 and `Invoice Date` == @datetime(2015, 2, 6)')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Filter Post and Pre test data\r\n",
    "post_data = weekly_gross_and_traffic.query('`Invoice Date` >= @datetime(2016, 4, 29)')\r\n",
    "pre_data = weekly_gross_and_traffic.query('`Invoice Date` >= @datetime(2015, 4, 29) and `Invoice Date` <= @datetime(2015, 7, 21)')\r\n",
    "\r\n",
    "pre_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# week gross per store\r\n",
    "\r\n",
    "# group pre data by store id\r\n",
    "_temp_df = pre_data.groupby(['StoreID'], as_index=False).sum()\r\n",
    "\r\n",
    "# Merge grouped pre gross and store data\r\n",
    "stores_columns = ['StoreID', 'Sq_Ft', 'AvgMonthSales', 'Region']\r\n",
    "weekly_gross_per_store = _temp_df.merge(stores_data[stores_columns], on='StoreID')\r\n",
    "\r\n",
    "# Add Treatment and Control Group\r\n",
    "_cond = weekly_gross_per_store['StoreID'].isin(treatment_stores_data['StoreID'])\r\n",
    "weekly_gross_per_store = weekly_gross_per_store.assign(Group = np.where(_cond, 'Treatment', 'Control'))\r\n",
    "\r\n",
    "weekly_gross_per_store.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Match Treatment and Control Units\r\n",
    "In this step, you should create the trend and seasonality variables, and use them along with you other control variable(s) to match two control units to each treatment unit. Treatment stores should be matched to control stores in the same region. Note: Calculate the number of transactions per store per week and use 12 periods to calculate trend and seasonality.  \r\n",
    "\r\n",
    "Apart from trend and seasonality...  \r\n",
    "\r\n",
    "-What control variables should be considered? Note: Only consider variables in the RoundRoastersStore file.  \r\n",
    "-What is the correlation between your each potential control variable and your performance metric? (Example of correlation matrix below)  \r\n",
    "-What control variables will you use to match treatment and control stores?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Correlation Gross Margin and Stores variables\r\n",
    "weekly_gross_per_store[['Gross Margin', 'Sq_Ft', 'AvgMonthSales']].corr().round(2)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "selected_variables = ['Trend', 'Seasonal', 'AvgMonthSales']\r\n",
    "selected_variables"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Analysis and Writeup\r\n",
    "Conduct your A/B analysis and create a short report outlining your results and recommendations.  \r\n",
    "\r\n",
    "In an AB Analysis we use the correlation matrix to find the most correlated variable to the performance metric to include in the AB controls tool to help find the best matches."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Match Treatment and Control Stores\r\n",
    "control_stores = weekly_gross_per_store.query('Group == \"Control\"')\r\n",
    "treatment_stores = weekly_gross_per_store.query('Group == \"Treatment\"')\r\n",
    "regions = ['Central', 'West']\r\n",
    "transformer = ColumnTransformer([('scaler', StandardScaler(), selected_variables)], remainder='drop')\r\n",
    "transformer.fit(control_stores)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Matched Treatment and Stores\r\n",
    "\r\n",
    "def matched_stores(control, treatment, regions_list):\r\n",
    "    control_store_ids = []\r\n",
    "    treatment_store_ids = []\r\n",
    "    store_id_region = []\r\n",
    "\r\n",
    "    for region in regions_list:\r\n",
    "        control_region = control.query('Region == @region')\r\n",
    "        treatment_region = treatment.query('Region == @region')\r\n",
    "        \r\n",
    "        kdtree_region = KDTree(transformer.transform(control_region), leaf_size=2)\r\n",
    "\r\n",
    "        for i, store_id in enumerate(treatment_region['StoreID']):\r\n",
    "            _data = treatment_region.iloc[i:i+1]\r\n",
    "            _idx = kdtree_region.query(transformer.transform(_data), k=2, return_distance=False)\r\n",
    "\r\n",
    "            treatment_store_ids += [store_id, store_id]\r\n",
    "            control_store_ids += list(control_region.iloc[_idx[0]]['StoreID'])\r\n",
    "            store_id_region += [region, region]\r\n",
    "\r\n",
    "    _merge_list = list(zip(treatment_store_ids, control_store_ids, store_id_region))\r\n",
    "    _columns_name = ['Treatment StoreID', 'Control StoreID', 'Region']\r\n",
    "    return pd.DataFrame(_merge_list, columns=_columns_name)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result_df = matched_stores(control_stores, treatment_stores, regions)\r\n",
    "result_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Match Pre and Post samples\r\n",
    "\r\n",
    "# Post Treatment Samples\r\n",
    "_treat_post = []\r\n",
    "for i in result_df['Treatment StoreID']:\r\n",
    "    _data_treat = post_data.query('StoreID == @i')\r\n",
    "    _treat_post = [_data_treat]\r\n",
    "\r\n",
    "post_treatment_sample = pd.concat(_treat_post)\r\n",
    "\r\n",
    "# Post Control Samples\r\n",
    "_cont_post = []\r\n",
    "for i in result_df['Control StoreID']:\r\n",
    "    _data_cont = post_data.query('StoreID == @i')\r\n",
    "    _cont_post = [_data_cont]\r\n",
    "\r\n",
    "post_control_sample = pd.concat(_cont_post)\r\n",
    "\r\n",
    "# Pre Treatment Samples\r\n",
    "_treat_pre = []\r\n",
    "for i in result_df['Treatment StoreID']:\r\n",
    "    _data_treat = pre_data.query('StoreID == @i')\r\n",
    "    _treat_pre = [_data_treat]\r\n",
    "\r\n",
    "pre_treatment_sample = pd.concat(_treat_pre)\r\n",
    "\r\n",
    "# Post Control Samples\r\n",
    "_cont_post = []\r\n",
    "for i in result_df['Control StoreID']:\r\n",
    "    _data_cont = pre_data.query('StoreID == @i')\r\n",
    "    _cont_post = [_data_cont]\r\n",
    "\r\n",
    "pre_control_sample = pd.concat(_cont_post)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "post_treatment_sample"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# is change not random? p < 0.05\r\n",
    "s, p = ttest_ind(post_treatment_sample['Gross Margin'], pre_treatment_sample['Gross Margin'])\r\n",
    "(1 - p) * 100"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}