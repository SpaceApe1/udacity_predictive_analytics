{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "5cba3963c37d400916a9e80ad5acd0c56daed180f4961ad2a064f19bb9bbda97"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# A/B Test: A New Menu Launch\n",
    "## Project Overview\n",
    "You're a business analyst for Round Roasters, a coffee restaurant in the United States of America. The executive team conducted a market test with a new menu and needs to figure whether the new menu can drive enough sales to offset the cost of marketing the new menu. Your job is to analyze the A/B test and write up a recommendation to whether the Round Roasters chain should launch this new menu."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load package\r\n",
    "from datetime import datetime, timedelta\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from scipy.stats import ttest_ind\r\n",
    "from sklearn.neighbors import KDTree\r\n",
    "from sklearn.compose import ColumnTransformer\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "\r\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "# plt.style.use('seaborn-whitegrid')\r\n",
    "plt.rcParams['figure.figsize'] = [12, 12]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Plan Your Analysis\n",
    "To perform the correct analysis, you will need to prepare a data set. Prior to rolling up your sleeves and preparing the data, it’s a good idea to have a plan of what you need to do in order to prepare the correct data set. A good plan will help you with your analysis. Here are a few questions to get you started:\n",
    "\n",
    "-What is the performance metric you’ll use to evaluate the results of your test?  \n",
    "-What is the test period?  \n",
    "-At what level (day, week, month, etc.) should the data be aggregated?  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load Stores data\r\n",
    "stores_data = pd.read_csv('round-roaster-stores.csv')\r\n",
    "stores_data.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stores_data.head(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load Transactions data\r\n",
    "# force Invoice Date column to datetime \r\n",
    "transactions_data = pd.read_csv('RoundRoastersTransactions.csv', parse_dates=['Invoice Date'])\r\n",
    "transactions_data.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "transactions_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load Treatment Stores data\r\n",
    "treatment_stores_data = pd.read_csv('treatment-stores.csv')\r\n",
    "treatment_stores_data.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "treatment_stores_data.head(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Clean Up Your Data\n",
    "In this step, you should prepare the data for steps 3 and 4. You should aggregate the transaction data to the appropriate level and filter on the appropriate data ranges. You can assume that there is no missing, incomplete, duplicate, or dirty data. You’re ready to move on to the next step when you have weekly transaction data for all stores."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Test cities: Denver and Chicago\r\n",
    "# Treatment: 12 Weeks [2016-April-29 to 2016-July-21], start on Friday\r\n",
    "# Control: 12 Weeks [(2015-April-29 to 2015-July-21], start on Wednesday\r\n",
    "# Total weeks to identify trend and season: 52 Weeks + treatment weeks + control weeks = 52 + 12 + 12 = 76 Weeks\r\n",
    "\r\n",
    "data_end_date = datetime(2016, 7, 21)\r\n",
    "data_start_date = data_end_date - timedelta(weeks=76)\r\n",
    "print(f'Data Start Date: {data_start_date} \\nData End Date: {data_end_date}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Filter data for further process\r\n",
    "filtered_transactions_data = transactions_data.query('`Invoice Date` > @data_start_date and `Invoice Date` <= @data_end_date')\r\n",
    "# Make Invoice Date Index\r\n",
    "filtered_transactions_data.set_index('Invoice Date', inplace=True)\r\n",
    "\r\n",
    "filtered_transactions_data.head(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Aggregate the data to get the weekly gross margin and weekly traffic count unique invoices\r\n",
    "weekly_gross_and_traffic = filtered_transactions_data.groupby([pd.Grouper(freq='W-FRI', closed='left'), 'StoreID'], as_index=True).agg({'Gross Margin': 'sum', 'Invoice Number': 'nunique'}).reset_index()\r\n",
    "weekly_gross_and_traffic.rename(columns={'Invoice Number': 'Weekly Foot Traffic'}, inplace=True)\r\n",
    "\r\n",
    "# Hack to start week on first date\r\n",
    "weekly_gross_and_traffic['Invoice Date'] = weekly_gross_and_traffic['Invoice Date'] - pd.offsets.Week(1)\r\n",
    "weekly_gross_and_traffic.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Find Trend and Seasonality\r\n",
    "stores_ID_list = list(weekly_gross_and_traffic['StoreID'].unique())\r\n",
    "df_list = []\r\n",
    "for store_id in stores_ID_list:\r\n",
    "    store_df = weekly_gross_and_traffic.query('StoreID == @store_id')\r\n",
    "    result = seasonal_decompose(store_df['Weekly Foot Traffic'].values, period=12, extrapolate_trend='freq')\r\n",
    "    store_df = store_df.assign(Trend = result.trend, Seasonal = result.seasonal)\r\n",
    "    df_list.append(store_df)\r\n",
    "\r\n",
    "cleaned_data = pd.concat(df_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# How many transactions did you get for Store 10018 in the week starting 2015-02-06?\r\n",
    "cleaned_data.query('StoreID == 10018 and `Invoice Date` == @datetime(2015, 2, 6)')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# aggregate the data by store ID\r\n",
    "cleaned_data = cleaned_data.groupby(['StoreID'], as_index=False).sum()\r\n",
    "\r\n",
    "# merge cleaned data with stores data\r\n",
    "stores_columns = ['StoreID', 'Sq_Ft', 'AvgMonthSales', 'Region']\r\n",
    "transactions_stores_merged= cleaned_data.merge(stores_data[stores_columns], on='StoreID')\r\n",
    "transactions_stores_merged['Group'] = np.where(transactions_stores_merged['StoreID'].isin(treatment_stores_data['StoreID'].values), 'Treatment', 'Control')\r\n",
    "\r\n",
    "transactions_stores_merged.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Match Treatment and Control Units\r\n",
    "In this step, you should create the trend and seasonality variables, and use them along with you other control variable(s) to match two control units to each treatment unit. Treatment stores should be matched to control stores in the same region. Note: Calculate the number of transactions per store per week and use 12 periods to calculate trend and seasonality.  \r\n",
    "\r\n",
    "Apart from trend and seasonality...  \r\n",
    "\r\n",
    "-What control variables should be considered? Note: Only consider variables in the RoundRoastersStore file.  \r\n",
    "-What is the correlation between your each potential control variable and your performance metric? (Example of correlation matrix below)  \r\n",
    "-What control variables will you use to match treatment and control stores?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Correlation between control variable and perfromance metric\r\n",
    "transactions_stores_merged.drop(columns='StoreID').corr().round(2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Correlated variables to be used in KDTree \r\n",
    "control_variables = ['Gross Margin', 'Weekly Foot Traffic', 'Trend', 'AvgMonthSales']\r\n",
    "\r\n",
    "control_variables"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# region data control\r\n",
    "control_data = transactions_stores_merged.query('Group == \"Control\"').reset_index().drop(columns='index')\r\n",
    "treatment_data = transactions_stores_merged.query('Group == \"Treatment\"').reset_index().drop(columns='index')\r\n",
    "\r\n",
    "west_region_control = control_data.query('Region == \"West\"').reset_index().drop(columns='index')\r\n",
    "central_region_control = control_data.query('Region == \"Central\"').reset_index().drop(columns='index')\r\n",
    "\r\n",
    "# region data treatment\r\n",
    "west_region_treatment = treatment_data.query('Region == \"West\"')\r\n",
    "central_region_treatment = treatment_data.query('Region == \"Central\"')\r\n",
    "\r\n",
    "# Scale the features\r\n",
    "transformer = ColumnTransformer([('scaler', StandardScaler(), control_variables)], remainder='drop')\r\n",
    "transformer.fit(control_data)\r\n",
    "\r\n",
    "# KDtree for west\r\n",
    "west_kdtree = KDTree(transformer.transform(west_region_control), leaf_size=2)\r\n",
    "# KDtree for central\r\n",
    "central_kdtree = KDTree(transformer.transform(central_region_control), leaf_size=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Find the neighbors\r\n",
    "\r\n",
    "def neighborStores(region_treatment_data, region_control_data, region_kdtree):\r\n",
    "    control_indexes = []\r\n",
    "    treatment_indexes = []\r\n",
    "    treatment_store_id = []\r\n",
    "    control_dist = []\r\n",
    "\r\n",
    "    for store_id in region_treatment_data['StoreID'].values:\r\n",
    "        treatment_store_id += [store_id, store_id]\r\n",
    "        store_id_data = region_treatment_data.query('StoreID == @store_id')\r\n",
    "        treatment_indexes.append(store_id_data.index.values[0])\r\n",
    "        dis, idx = region_kdtree.query(transformer.transform(store_id_data), k=2)\r\n",
    "        control_dist += list(dis[0])\r\n",
    "        control_indexes += list(idx[0])\r\n",
    "    \r\n",
    "    control_store_id = list(region_control_data.iloc[control_indexes]['StoreID'].values)\r\n",
    "    treat_cont_dist_df = pd.DataFrame(list(zip(treatment_store_id, control_store_id, control_dist)), columns=['Treatment', 'Control', 'Distance'])\r\n",
    "\r\n",
    "    return treat_cont_dist_df, control_indexes, treatment_indexes\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "west_df, west_control_indexes, west_treatment_indexes = neighborStores(west_region_treatment, west_region_control, west_kdtree)\r\n",
    "central_df, central_control_indexes, central_treatment_indexes = neighborStores(central_region_treatment, central_region_control, central_kdtree)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "west_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "central_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Analysis and Writeup\r\n",
    "Conduct your A/B analysis and create a short report outlining your results and recommendations.  \r\n",
    "\r\n",
    "In an AB Analysis we use the correlation matrix to find the most correlated variable to the performance metric to include in the AB controls tool to help find the best matches."
   ],
   "metadata": {}
  }
 ]
}