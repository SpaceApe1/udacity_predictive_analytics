{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "5cba3963c37d400916a9e80ad5acd0c56daed180f4961ad2a064f19bb9bbda97"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# A/B Test: A New Menu Launch\n",
    "## Project Overview\n",
    "You're a business analyst for Round Roasters, a coffee restaurant in the United States of America. The executive team conducted a market test with a new menu and needs to figure whether the new menu can drive enough sales to offset the cost of marketing the new menu. Your job is to analyze the A/B test and write up a recommendation to whether the Round Roasters chain should launch this new menu."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load package\r\n",
    "from datetime import datetime, timedelta\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from scipy.stats import ttest_ind, ttest_rel\r\n",
    "from sklearn.neighbors import KDTree\r\n",
    "from sklearn.compose import ColumnTransformer\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "\r\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "# plt.style.use('seaborn-whitegrid')\r\n",
    "plt.rcParams['figure.figsize'] = [12, 12]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Plan Your Analysis\n",
    "To perform the correct analysis, you will need to prepare a data set. Prior to rolling up your sleeves and preparing the data, it’s a good idea to have a plan of what you need to do in order to prepare the correct data set. A good plan will help you with your analysis. Here are a few questions to get you started:\n",
    "\n",
    "-What is the performance metric you’ll use to evaluate the results of your test?  \n",
    "-What is the test period?  \n",
    "-At what level (day, week, month, etc.) should the data be aggregated?  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load Stores data\r\n",
    "stores_data = pd.read_csv('round-roaster-stores.csv')\r\n",
    "stores_data.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stores_data.head(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load Transactions data\r\n",
    "# force Invoice Date column to datetime \r\n",
    "transactions_data = pd.read_csv('RoundRoastersTransactions.csv', parse_dates=['Invoice Date'])\r\n",
    "transactions_data.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "transactions_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load Treatment Stores data\r\n",
    "treatment_stores_data = pd.read_csv('treatment-stores.csv')\r\n",
    "treatment_stores_data.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "treatment_stores_data.head(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Clean Up Your Data\n",
    "In this step, you should prepare the data for steps 3 and 4. You should aggregate the transaction data to the appropriate level and filter on the appropriate data ranges. You can assume that there is no missing, incomplete, duplicate, or dirty data. You’re ready to move on to the next step when you have weekly transaction data for all stores."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Test cities: Denver and Chicago\r\n",
    "# Treatment: 12 Weeks [2016-April-29 to 2016-July-21], start on Friday\r\n",
    "# Control: 12 Weeks [(2015-April-29 to 2015-July-21], start on Wednesday\r\n",
    "# Total weeks to identify trend and season: 52 Weeks + treatment weeks + control weeks = 52 + 12 + 12 = 76 Weeks\r\n",
    "\r\n",
    "data_end_date = datetime(2016, 7, 21)\r\n",
    "data_start_date = data_end_date - timedelta(weeks=76)\r\n",
    "print(f'Data Start Date: {data_start_date} \\nData End Date: {data_end_date}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Filter data for further process\r\n",
    "_trans = transactions_data.query('`Invoice Date` > @data_start_date and `Invoice Date` <= @data_end_date')\r\n",
    "# Make Invoice Date Index\r\n",
    "_trans.set_index('Invoice Date', inplace=True)\r\n",
    "\r\n",
    "# Aggregate the data to get the weekly gross margin and weekly traffic count unique invoices\r\n",
    "agg_rules = {'Gross Margin': 'sum', 'Invoice Number': 'nunique'}\r\n",
    "_trans = _trans.groupby([pd.Grouper(freq='W-FRI', closed='left'), 'StoreID']).agg(agg_rules).reset_index()\r\n",
    "_trans.rename(columns={'Invoice Number': 'Weekly Foot Traffic'}, inplace=True)\r\n",
    "\r\n",
    "# Hack to start week on first date\r\n",
    "_trans['Invoice Date'] = _trans['Invoice Date'] - pd.offsets.Week(1)\r\n",
    "\r\n",
    "# Create Trend and Seasonal\r\n",
    "result = seasonal_decompose(_trans['Weekly Foot Traffic'], period=12, extrapolate_trend='freq')\r\n",
    "\r\n",
    "# Add Trend and Seasonal weekly gross data\r\n",
    "_trans = _trans.assign(Trend = result.trend, Seasonal = result.seasonal)\r\n",
    "\r\n",
    "# join weekly transactions and stores data \r\n",
    "stores_columns = ['StoreID', 'Sq_Ft', 'AvgMonthSales', 'Region']\r\n",
    "_trans = _trans.merge(stores_data[stores_columns], on='StoreID')\r\n",
    "\r\n",
    "# Add group variables to merged data\r\n",
    "_is_treatment = _trans['StoreID'].isin(treatment_stores_data['StoreID'])\r\n",
    "weekly_gross_and_traffic = _trans.assign(Group = np.where(_is_treatment, 'Treatment', 'Control'))\r\n",
    "\r\n",
    "# Test the progress\r\n",
    "weekly_gross_and_traffic.query('StoreID == 10018 and `Invoice Date` == @datetime(2015, 2, 6)')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Filter Post and Pre test data\r\n",
    "post_data = weekly_gross_and_traffic.query('`Invoice Date` >= @datetime(2016, 4, 29)')\r\n",
    "pre_data = weekly_gross_and_traffic.query('`Invoice Date` >= @datetime(2015, 4, 29) and `Invoice Date` <= @datetime(2015, 7, 21)')\r\n",
    "\r\n",
    "pre_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Aggregate Pre and Post data per store\r\n",
    "agg_store_rules = {\r\n",
    "    'Gross Margin': 'sum', \r\n",
    "    'Weekly Foot Traffic': 'sum', \r\n",
    "    'Trend': 'sum', \r\n",
    "    'Seasonal': 'sum', \r\n",
    "    'Sq_Ft': 'first',\r\n",
    "    'AvgMonthSales': 'first',\r\n",
    "    'Region': 'first',\r\n",
    "    'Group': 'first'\r\n",
    "}\r\n",
    "\r\n",
    "def agg_per_store(df, agg_rule = agg_store_rules):\r\n",
    "    return df.groupby(['StoreID'], as_index=False).agg(agg_rule)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Pre Data per store\r\n",
    "pre_data_per_store = agg_per_store(pre_data)\r\n",
    "\r\n",
    "# Post Data per store\r\n",
    "post_data_per_store = agg_per_store(post_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "post_data_per_store.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Match Treatment and Control Units\r\n",
    "In this step, you should create the trend and seasonality variables, and use them along with you other control variable(s) to match two control units to each treatment unit. Treatment stores should be matched to control stores in the same region. Note: Calculate the number of transactions per store per week and use 12 periods to calculate trend and seasonality.  \r\n",
    "\r\n",
    "Apart from trend and seasonality...  \r\n",
    "\r\n",
    "-What control variables should be considered? Note: Only consider variables in the RoundRoastersStore file.  \r\n",
    "-What is the correlation between your each potential control variable and your performance metric? (Example of correlation matrix below)  \r\n",
    "-What control variables will you use to match treatment and control stores?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Correlation Gross Margin and Stores variables\r\n",
    "pre_data_per_store[['Gross Margin', 'Sq_Ft', 'AvgMonthSales']].corr().round(2)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "selected_variables = ['Trend', 'Seasonal', 'AvgMonthSales']\r\n",
    "selected_variables"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Analysis and Writeup\r\n",
    "Conduct your A/B analysis and create a short report outlining your results and recommendations.  \r\n",
    "\r\n",
    "In an AB Analysis we use the correlation matrix to find the most correlated variable to the performance metric to include in the AB controls tool to help find the best matches."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Match Pre Treatment and Control Stores Data\r\n",
    "pre_control_stores = pre_data_per_store.query('Group == \"Control\"').reset_index(drop=True)\r\n",
    "pre_treatment_stores = pre_data_per_store.query('Group == \"Treatment\"').reset_index(drop=True)\r\n",
    "\r\n",
    "# filter region data\r\n",
    "# Region: 'Central'\r\n",
    "pre_control_central = pre_control_stores.query('Region == \"Central\"').reset_index(drop=True)\r\n",
    "pre_treatment_central = pre_treatment_stores.query('Region == \"Central\"').reset_index(drop=True)\r\n",
    "\r\n",
    "# Region: 'West'\r\n",
    "pre_control_west = pre_control_stores.query('Region == \"West\"').reset_index(drop=True)\r\n",
    "pre_treatment_west = pre_treatment_stores.query('Region == \"West\"').reset_index(drop=True)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Scale Data\r\n",
    "transformer = ColumnTransformer([('scaler', StandardScaler(), selected_variables)], remainder='drop')\r\n",
    "transformer.fit(pre_control_stores)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Matched Treatment and Stores - version: 2\r\n",
    "# return: 'Treatment store ID', 'Control store ID', 'Pre Gross Treatment', 'Pre Gross Control'\r\n",
    "\r\n",
    "def match_treatment_control_stores(region_control_data, region_treatment_data):\r\n",
    "    treatment_ids = []\r\n",
    "    control_ids = []\r\n",
    "    treatment_store_gross = []\r\n",
    "    control_store_gross = []\r\n",
    "\r\n",
    "    kdtree = KDTree(transformer.transform(region_control_data), leaf_size=2)\r\n",
    "\r\n",
    "    for i , treat_store_id in enumerate(region_treatment_data['StoreID']):\r\n",
    "        treatment_ids += [treat_store_id, treat_store_id]\r\n",
    "        treat_data = region_treatment_data.iloc[i:i+1]\r\n",
    "        \r\n",
    "        _tg = treat_data.iloc[0]['Gross Margin'] # Get column value from df with single row\r\n",
    "        treatment_store_gross += [_tg,_tg]\r\n",
    "        \r\n",
    "        idx = kdtree.query(transformer.transform(treat_data), k=2, return_distance=False, dualtree=True)\r\n",
    "\r\n",
    "        _matched = region_control_data.iloc[idx[0]]\r\n",
    "        control_ids += list(_matched['StoreID'])\r\n",
    "        control_store_gross += list(_matched['Gross Margin'])\r\n",
    "    \r\n",
    "    _x = list(zip(treatment_ids, control_ids, treatment_store_gross, control_store_gross))\r\n",
    "    _columns = ['Treatment StoreID', 'Control StoreID', 'Treatment Pre Gross', 'Control Pre Gross']\r\n",
    "    return pd.DataFrame(_x, columns=_columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Find Matched Stores for Central and West Region\r\n",
    "stores_matched_west = match_treatment_control_stores(pre_control_west, pre_treatment_west)\r\n",
    "stores_matched_central = match_treatment_control_stores(pre_control_central, pre_treatment_central)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Join Post Data to region region matched stores\r\n",
    "\r\n",
    "def pre_post_data_report(matched_stores, post_stores_data):\r\n",
    "    post_column = ['StoreID', 'Gross Margin']\r\n",
    "    _psd = post_stores_data[post_column].set_index('StoreID')\r\n",
    "\r\n",
    "    # treatment\r\n",
    "    _res = matched_stores.join(_psd, on='Treatment StoreID').rename(columns={'Gross Margin': 'Treatment Post Gross'})\r\n",
    "    # control\r\n",
    "    _res = _res.join(_p_d, on='Control StoreID').rename(columns={'Gross Margin': 'Control Post Gross'})\r\n",
    "\r\n",
    "    return _res\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pre_post_data_report(stores_matched_central, post_data_per_store)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TODO\r\n",
    "# TTest to find if the pre and post mean of treatments are not random\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TODO\r\n",
    "# compare the changes from pre and post between treatment and control\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}