{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd05cba3963c37d400916a9e80ad5acd0c56daed180f4961ad2a064f19bb9bbda97",
   "display_name": "Python 3.9.2  ('.venv': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "5cba3963c37d400916a9e80ad5acd0c56daed180f4961ad2a064f19bb9bbda97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Project: Creditworthiness"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [13, 13]"
   ]
  },
  {
   "source": [
    "## Step 1: Business and Data Understanding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load past applications\n",
    "past_applications = pd.read_excel('credit-data-training.xlsx')\n",
    "new_customers = pd.read_excel('customers-to-score.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_applications.head()"
   ]
  },
  {
   "source": [
    "### Key Decisions:\n",
    "\n",
    "* What decisions needs to be made?\n",
    "  * I need to evaluate the creditworthiness of the new 500 loan applicants.\n",
    "\n",
    "* What data is needed to inform those decisions?\n",
    "  * I need past loan applicant's information on credit application results and the data used to rate those results like Duration of credit, credit amount, installment, age of the applicant, etc.\n",
    "\n",
    "* What kind of model (Continuous, Binary, Non-Binary, Time-Series) do we need to use to help make these decisions?\n",
    "  * The model type will be Binary as I will be predicting an applicant to be either creditworthy or non-creditworthy.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Step 2: Building the Training Set\n",
    "\n",
    "### Guidelines:\n",
    "* For numerical data fields, are there any fields that highly-correlate with each other? The correlation should be at least .70 to be considered “high”.\n",
    "* Are there any missing data for each of the data fields? Fields with a lot of missing data should be removed\n",
    "* Are there only a few values in a subset of your data field? Does the data field look very uniform (there is only one value for the entire field?). This is called “low variability” and you should remove fields that have low variability. Refer to the \"Tips\" section to find examples of data fields with low-variability.\n",
    "*Your clean data set should have 13 columns where the Average of Age Years should be 36 (rounded up)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables Non Null Count\n",
    "past_applications.info()\n",
    "\n",
    "columns_to_drop = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Vizualization\n",
    "fig, axes = plt.subplots(4,5, figsize=(23, 23))\n",
    "x = list(past_applications.columns)\n",
    "\n",
    "for i, column in enumerate(past_applications.columns):\n",
    "    if past_applications[column].dtype == np.dtype('O'):\n",
    "        past_applications[column].value_counts().plot(kind='bar', rot=0, ax=axes[int(i/5)][i%5]).set_title(column)\n",
    "    else:\n",
    "        past_applications[column].hist(ax=axes[int(i/5)][i%5]).set_title(column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop Duration-in-Current-address due to many missing data\n",
    "columns_to_drop.append('Duration-in-Current-address')\n",
    "# drop Concurrent-Credits due to low variability\n",
    "columns_to_drop.append('Concurrent-Credits')\n",
    "# drop Occupation due to low variability\n",
    "columns_to_drop.append('Occupation')\n",
    "\n",
    "# drop due to low variability\n",
    "columns_to_drop.append('Guarantors')\n",
    "columns_to_drop.append('Type-of-apartment')\n",
    "columns_to_drop.append('No-of-dependents')\n",
    "columns_to_drop.append('Foreign-Worker')\n",
    "\n",
    "clean_data = past_applications.drop(columns=columns_to_drop)\n",
    "\n",
    "past_applications[columns_to_drop].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Removed Vizualization\n",
    "fig, axes = plt.subplots(2,4, figsize=(15, 9))\n",
    "\n",
    "for i, column in enumerate(columns_to_drop):\n",
    "    if past_applications[column].dtype == np.dtype('O'):\n",
    "        past_applications[column].value_counts().plot(kind='bar', rot=0, ax=axes[int(i/4)][i%4]).set_title(column)\n",
    "    else:\n",
    "        past_applications[column].hist(ax=axes[int(i/4)][i%4]).set_title(column)\n",
    "\n",
    "fig.savefig('droped_variables_graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation\n",
    "clean_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "clean_data = clean_data.fillna(clean_data.mean())\n",
    "clean_data.mean().round(0)"
   ]
  },
  {
   "source": [
    "### Answer this question:\n",
    "\n",
    "* In your cleanup process, which fields did you remove or impute? Please justify why you removed or imputed these fields. Visualizations are encouraged.\n",
    "  * The imputed field is Age-years, There 12 applicants with empty age data. I can not remove these applicants as I will lose 2.4% of the data. I will fill all empty data with an age average of 36.\n",
    "  * I will remove all fields with low variability to remove bias in my model. The removed fields are:\n",
    "    - Duration in a current address\n",
    "    - Concurrent credits\n",
    "    - Occupation\n",
    "    - Guarantors\n",
    "    - Type of apartment\n",
    "    - No of dependents\n",
    "    - Foreign worker"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Step 3: Train your Classification Models\n",
    "\n",
    "First, create your Estimation and Validation samples where 70% of your dataset should go to Estimation and 30% of your entire dataset should be reserved for Validation. Set the Random Seed to 1.\n",
    "\n",
    "Create all of the following models: Logistic Regression, Decision Tree, Forest Model, Boosted Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace target to binary\n",
    "target_column = 'Credit-Application-Result'\n",
    "target_label = ['Creditworthy', 'Non-Creditworthy'] # list(clean_data[target_column].unique())\n",
    "clean_data[target_column].replace({'Creditworthy': 0, 'Non-Creditworthy': 1}, inplace=True)\n",
    "\n",
    "# Categorical Columns\n",
    "categorical_columns = ['Account-Balance', 'Payment-Status-of-Previous-Credit', 'Purpose', 'Value-Savings-Stocks', 'Length-of-current-employment', 'No-of-Credits-at-this-Bank']\n",
    "prefix = ['AccountB', 'PaymentSPC', 'Purpose', 'ValueSS', 'LengthCE', 'NumberCB']\n",
    "\n",
    "# convert categorical varibles into dummy [indicator variables]\n",
    "clean_data_with_dummies = pd.get_dummies(clean_data, prefix=prefix, columns=categorical_columns, drop_first=False)\n",
    "\n",
    "clean_data_with_dummies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data set to train and test subsets\n",
    "train, test = train_test_split(clean_data_with_dummies, test_size=0.3, random_state=1)\n",
    "\n",
    "# Training Data\n",
    "Y_train = train['Credit-Application-Result']\n",
    "X_train = train.drop(columns='Credit-Application-Result')\n",
    "\n",
    "# Test Data\n",
    "Y_test = test['Credit-Application-Result']\n",
    "X_test = test.drop(columns='Credit-Application-Result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forest Model Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosted Tree Model\n"
   ]
  },
  {
   "source": [
    "### Answer these questions for each model you created:\n",
    "\n",
    "* Which predictor variables are significant or the most important? Please show the p-values or variable importance charts for all of your predictor variables.\n",
    "\n",
    "* Validate your model against the Validation set. What was the overall percent accuracy? Show the confusion matrix. Are there any bias seen in the model’s predictions? \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Step 4: Writeup\n",
    "\n",
    "Decide on the best model and score your new customers. For reviewing consistency, if Score_Creditworthy is greater than Score_NonCreditworthy, the person should be labeled as “Creditworthy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Answer these questions\n",
    "* Which model did you choose to use? \n",
    "* How many individuals are creditworthy?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}