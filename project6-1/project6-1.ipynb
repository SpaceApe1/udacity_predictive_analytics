{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Combining Predictive Techniques"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Given\r\n",
    "\r\n",
    "* StoreSalesData.csv - This file contains sales by product category for all existing stores for 2012, 2013, and 2014.\r\n",
    "* StoreInformation.csv - This file contains location data for each of the stores.\r\n",
    "* StoreDemographicData.csv - This file contains demographic data for the areas surrounding each of the existing stores and locations for new stores."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load Package"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from sklearn.cluster import KMeans\r\n",
    "from sklearn.preprocessing import MinMaxScaler\r\n",
    "from sklearn.metrics import calinski_harabasz_score, silhouette_score, davies_bouldin_score\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.compose import ColumnTransformer\r\n",
    "from sklearn.tree import DecisionTreeClassifier\r\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\r\n",
    "\r\n",
    "from statsmodels.tsa.exponential_smoothing.ets import ETSModel\r\n",
    "from statsmodels.tsa.arima.model import ARIMA\r\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\r\n",
    "from statsmodels.tools.eval_measures import rmse\r\n",
    "from statsmodels.graphics import tsaplots\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "# plt.style.use('seaborn-whitegrid')\r\n",
    "plt.rcParams['figure.figsize'] = [11, 7]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load Stores Sales\r\n",
    "stores_sales_data = pd.read_csv('storesalesdata.csv')\r\n",
    "# Bad Data: There no date 29-Feb-2014, Drod the data\r\n",
    "# stores_sales_data = stores_sales_data.query('Date != \"2014 02 29\"')\r\n",
    "# Convert Date varible to daterime object\r\n",
    "# stores_sales_data = stores_sales_data.assign(Date = pd.to_datetime(stores_sales_data['Date']))\r\n",
    "\r\n",
    "stores_sales_data.head(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load Store Information\r\n",
    "store_information_data = pd.read_csv('storeinformation.csv')\r\n",
    "store_information_data.head(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load Store Demographic Data\r\n",
    "store_demographic_data = pd.read_csv('storedemographicdata.csv')\r\n",
    "store_demographic_data.head(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 1: Store Format (segments) for Existing Stores\r\n",
    "\r\n",
    "To remedy the product surplus and shortages, the company wants to introduce different store formats. Each store format will have a different product selection in order to better match local demand. The actual building sizes will not change, just the product selection and internal layouts.\r\n",
    "\r\n",
    "* Determine the optimal number of store formats based on sales data.\r\n",
    "    - Sum sales data by StoreID and Year\r\n",
    "    - Use percentage sales per category per store for clustering (category sales as a percentage of total store sales).\r\n",
    "    - Use only 2015 sales data.\r\n",
    "    - Use a K-means clustering model.\r\n",
    "\r\n",
    "* Segment the 85 current stores into the different store formats.\r\n",
    "* Use the StoreSalesData.csv and StoreInformation.csv files.\r\n",
    "\r\n",
    "## Task 1 Submission\r\n",
    "1. What is the optimal number of store formats? How did you arrive at that number?\r\n",
    "2. How many stores fall into each store format?\r\n",
    "3. Based on the results of the clustering model, what is one way that the clusters differ from one another?\r\n",
    "4. Please provide a map created in Tableau that shows the location of the existing stores, uses color to show cluster, and size to show total sales. Make sure to include a legend! Feel free to simply copy and paste the map into the submission template."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Aggregate sum of sales by Store and Year\r\n",
    "filtered_columns = ['Dry_Grocery', 'Dairy', 'Frozen_Food', 'Meat', 'Produce', 'Floral', 'Deli', 'Bakery', 'General_Merchandise']\r\n",
    "filtered_stores_data =  stores_sales_data.groupby(['Store', 'Year'], as_index=False)[filtered_columns].sum()\r\n",
    "# Add Store Total Sales per year\r\n",
    "filtered_stores_data = filtered_stores_data.assign(Total_Sales = filtered_stores_data[filtered_columns].sum(axis=1))\r\n",
    "# Calculate percentage sales per category per store\r\n",
    "filtered_stores_data[filtered_columns] = filtered_stores_data[filtered_columns].div(filtered_stores_data['Total_Sales'], axis=0)\r\n",
    "\r\n",
    "# Filter 2015 data\r\n",
    "filtered_stores_sales_2015_data = filtered_stores_data.query('Year == 2015')\r\n",
    "\r\n",
    "print('\\nFiltered and Aggregated 2015 Store Data')\r\n",
    "filtered_stores_sales_2015_data.head()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Find best numbers of clusters\r\n",
    "\r\n",
    "# Function to return pandas describe\r\n",
    "def cluster_number_test(raw_data, score_metric, number_test, cluster_number_list):\r\n",
    "    df_list = []\r\n",
    "    scaler = MinMaxScaler().fit_transform(raw_data)\r\n",
    "\r\n",
    "    for i in cluster_number_list:\r\n",
    "        score_list = []\r\n",
    "        for _ in range(number_test):\r\n",
    "            kmeans = KMeans(n_clusters=i)\r\n",
    "            kmeans.fit(scaler)\r\n",
    "            score = score_metric(raw_data, kmeans.labels_)\r\n",
    "            score_list.append(score)\r\n",
    "\r\n",
    "        temp_df = pd.DataFrame(score_list)\r\n",
    "        df_list.append(temp_df)\r\n",
    "\r\n",
    "    column_names = [f'Cluster {i}' for i in cluster_number_list]\r\n",
    "    _df = pd.concat(df_list, axis=1)\r\n",
    "    _df.columns = column_names\r\n",
    "    return _df.describe().round(2)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Finding best number of cluster\r\n",
    "raw_data = filtered_stores_sales_2015_data[filtered_columns]\r\n",
    "test_count = 100\r\n",
    "possible_clusters = range(2,8)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Run Test\r\n",
    "c_h_score = cluster_number_test(raw_data, calinski_harabasz_score, test_count, possible_clusters)\r\n",
    "sil_score = cluster_number_test(raw_data, silhouette_score, test_count, possible_clusters)\r\n",
    "d_b_score = cluster_number_test(raw_data, davies_bouldin_score, test_count, possible_clusters)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Metric: Calinski Harabasz Score - Higher the better\r\n",
    "print(f'Run Calinski Harabasz Score Test {test_count} times - Higher the better')\r\n",
    "print(c_h_score, '\\n')\r\n",
    "# Metric: Silhoutte Score - Higher the better\r\n",
    "print(f'Run Silhoutte Score Test {test_count} times - Higher the better')\r\n",
    "print(sil_score, '\\n')\r\n",
    "# Metric: Davies Bouldin Score - Smaller the better\r\n",
    "print(f'Run Davies Bouldin Score Test {test_count} times - Smaller the better')\r\n",
    "print(d_b_score)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# From above test, what is the best number of cluster?\r\n",
    "\r\n",
    "##### Score Results #####\r\n",
    "# Calinski Harabasz Score: 2 Cluster\r\n",
    "# Silhoutte Score: 3 Cluster\r\n",
    "# Davies Bouldin Score: 3 Cluster\r\n",
    "\r\n",
    "best_number_cluster = 3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Clusters - number of clusters = 3 \r\n",
    "kmeans = KMeans(n_clusters = best_number_cluster)\r\n",
    "# scale data\r\n",
    "scaled_data = MinMaxScaler().fit_transform(raw_data)\r\n",
    "kmeans.fit(scaled_data)\r\n",
    "# Add cluser laber to data\r\n",
    "filtered_stores_sales_2015_data = filtered_stores_sales_2015_data.assign(Segment = kmeans.labels_)\r\n",
    "\r\n",
    "# check numbers of stores in each Segment\r\n",
    "print('\\nNumber of stores in Segment')\r\n",
    "print(filtered_stores_sales_2015_data['Segment'].value_counts())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Cluster interpretion: Use centroid\r\n",
    "columns_name = ['Dry_Grocery', 'Dairy', 'Frozen_Food', 'Meat', 'Produce', 'Floral', 'Deli', 'Bakery', 'General_Merchandise']\r\n",
    "print('\\nAvarage distance between store formats')\r\n",
    "pd.DataFrame(kmeans.cluster_centers_, columns=columns_name).round(4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Merge filtered store sale with store\r\n",
    "filtered_stores_sales_2015_merged_information_data = filtered_stores_sales_2015_data.merge(store_information_data, how='inner', on='Store')\r\n",
    "filtered_stores_sales_2015_merged_information_data.head(2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import plotly\r\n",
    "from plotly import express as px, io as pio\r\n",
    "pio.renderers.default = \"notebook_connected\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TODO\r\n",
    "# Plot Clusters\r\n",
    "# Scatter Markers Customizing: Size, Color, Shape, Transparency\r\n",
    "\r\n",
    "# filtered_stores_sales_2015_merged_information_data.to_csv('filtered_stores_sales_2015_merged_information_data.csv')\r\n",
    "# use above data to plot cluster map in Tableau\r\n",
    "# import plotly\r\n",
    "from plotly import express as px, io as pio\r\n",
    "pio.renderers.default = \"notebook_connected\"\r\n",
    "from geopy.geocoders import Nominatim\r\n",
    "from geopy.extra.rate_limiter import RateLimiter\r\n",
    "\r\n",
    "df_loc = filtered_stores_sales_2015_merged_information_data.assign(Address_City = lambda x: x.Address + ', '+x.City)\r\n",
    "df_loc.drop(columns=['Address', 'City'], inplace=True)\r\n",
    "locator = Nominatim(user_agent='myGeocoder')\r\n",
    "geocode = RateLimiter(locator.geocode, min_delay_seconds=2)\r\n",
    "df_loc['Location'] = df_loc['Address_City'].apply(geocode)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = df_loc['Location'].apply(lambda loc: tuple(loc.point) if loc else None)\r\n",
    "df_loc[['latitude', 'longitude', 'altitude']] = pd.DataFrame(x.to_list(), index=df_loc.index)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = px.scatter_geo(df_loc, lat='latitude', lon='longitude', color='Segment', hover_name='Store', size='Total_Sales', size_max=15, fitbounds='locations')\r\n",
    "fig.write_html('scatter_map.html')\r\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 2: Store Format for New Stores\r\n",
    "\r\n",
    "The grocery store chain has 10 new stores opening up at the beginning of the year. The company wants to determine which store format each of the new stores should have. However, we don’t have sales data for these new stores yet, so we’ll have to determine the format using each of the new store’s demographic data.\r\n",
    "\r\n",
    "You’ve been asked to:\r\n",
    "\r\n",
    "* Develop a model that predicts which segment a store falls into based on the demographic and socioeconomic characteristics of the population that resides in the area around each new store.\r\n",
    "* Use a 20% validation sample with Random Seed = 3 when creating samples with which to compare the accuracy of the models. Make sure to compare a decision tree, forest, and boosted model.\r\n",
    "* Use the model to predict the best store format for each of the 10 new stores.\r\n",
    "* Use the StoreDemographicData.csv file, which contains the information for the area around each store.\r\n",
    "\r\n",
    "Note: In a real world scenario, you could use PCA to reduce the number of predictor variables. However, there is no need to do so in this project. You can leave all predictor variables in the model.\r\n",
    "\r\n",
    "\r\n",
    "## Task 2 Submission\r\n",
    "* What methodology did you use to predict the best store format for the new stores? Why did you choose that methodology?\r\n",
    "* What are the three most important variables that help explain the relationship between demographic indicators and store formats? Please include a visualization.\r\n",
    "* What format do each of the 10 new stores fall into? Please provide a data table.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 44 variables in demographic store data\r\n",
    "store_demographic_columns = ['Age0to9', 'Age10to17', 'Age18to24', 'Age25to29', 'Age30to39', 'Age40to49', 'Age50to64', 'Age65Plus', 'EdLTHS', 'EdHSGrad', 'EdSomeCol', 'EdAssociate', 'EdBachelor', 'EdMaster', 'EdProfSchl', 'EdDoctorate', 'HHSz1Per', 'HHSz2Per', 'HHSz3Per', 'HHSz4Per', 'HHSz5PlusPer', 'HHIncU25K', 'HHInc25Kto50K', 'HHInc50Kto75K',\r\n",
    "       'HHInc75Kto100K', 'HHInc100Kto150K', 'HHInc150Kto250K', 'HHInc250KPlus', 'PopAsian', 'PopBlack', 'PopHispanic', 'PopMulti', 'PopNativeAmer', 'PopOther', 'PopPacIsl', 'PopWhite', 'HVal0to100K', 'HVal100Kto200K', 'HVal200Kto300K', 'HVal300Kto400K', 'HVal400Kto500K', 'HVal500Kto750K', 'HVal750KPlus', 'PopDens']\r\n",
    "\r\n",
    "# join demographic data with store information\r\n",
    "store_info_columns = ['Store', 'Type']\r\n",
    "store_demographic_with_info_data = store_demographic_data.merge(store_information_data[store_info_columns], on='Store')\r\n",
    "\r\n",
    "#  filter existing store\r\n",
    "store_demographic_with_info_data_existing = store_demographic_with_info_data.query('Type == \"Existing\"')\r\n",
    "#  filter new store\r\n",
    "store_demographic_with_info_data_new = store_demographic_with_info_data.query('Type == \"New\"')\r\n",
    "\r\n",
    "# merge segment to existing store\r\n",
    "store_filtered_columns = ['Store', 'Segment']\r\n",
    "_temp_df = filtered_stores_sales_2015_merged_information_data[store_filtered_columns]\r\n",
    "store_demographic_with_info_data_existing = store_demographic_with_info_data_existing.merge(_temp_df, on='Store')\r\n",
    "\r\n",
    "# Prepair X and y for training\r\n",
    "y = store_demographic_with_info_data_existing['Segment']\r\n",
    "X = store_demographic_with_info_data_existing.drop(columns='Segment')\r\n",
    "# Split train and test data\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=3)\r\n",
    "# Column Transformer\r\n",
    "column_transformer = ColumnTransformer([('numerical', MinMaxScaler(), store_demographic_columns)])\r\n",
    "column_transformer.fit(X_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Classifier Score on N run test\r\n",
    "# Transform Train Data\r\n",
    "X_train_transformed = column_transformer.transform(X_train)\r\n",
    "# Transform Test Data\r\n",
    "X_test_transformed = column_transformer.transform(X_test)\r\n",
    "\r\n",
    "def classifier_test_score(estimator, X_train=X_train_transformed, y_train=y_train, X_test=X_test_transformed, y_test=y_test, cv=100):\r\n",
    "    _score_list = []\r\n",
    "    for _ in range(cv):\r\n",
    "        _cls = estimator().fit(X_train, y_train)\r\n",
    "        _score = _cls.score(X_test, y_test)\r\n",
    "        _score_list.append(_score)\r\n",
    "    return np.array(_score_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Decision Tree Classifier \r\n",
    "decision_tree_result = classifier_test_score(DecisionTreeClassifier)\r\n",
    "# Random Forest Classifier\r\n",
    "random_forest_result = classifier_test_score(RandomForestClassifier)\r\n",
    "# Gradient Boosting Classifier\r\n",
    "gradient_boosting = classifier_test_score(GradientBoostingClassifier)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Model Score\r\n",
    "print('Model Accuracy Score in Validation Data')\r\n",
    "print(f'Decision Tree Average Accuracy Score in Validation Data: {decision_tree_result.mean().round(2)}')\r\n",
    "print(f'Random Forest Average Accuracy Score in Validation Data: {random_forest_result.mean().round(2)}')\r\n",
    "print(f'Gradient Boosting Accuracy Average Score in Validation Data: {gradient_boosting.mean().round(2)}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot Feature Importance: Decision Tree\r\n",
    "# Score\r\n",
    "decision_tree_model = DecisionTreeClassifier().fit(X_train_transformed, y_train)\r\n",
    "print(f'Decision Tree Score in Validation Data: {decision_tree_model.score(X_test_transformed, y_test).round(2)}')\r\n",
    "# Feature Importances\r\n",
    "feature_imp_dt = pd.Series(decision_tree_model.feature_importances_, index=store_demographic_columns).sort_values()\r\n",
    "print('Top 3 features:', list(feature_imp_dt[-3:].index)[::-1])\r\n",
    "# Plot\r\n",
    "feature_imp_dt.plot(kind='barh', title='Decision Tree: Feature Importances')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot Feature Importance: Random Forest\r\n",
    "# Score\r\n",
    "random_forest_model = RandomForestClassifier().fit(X_train_transformed, y_train)\r\n",
    "print(f'Random Forest Score in Validation Data: {random_forest_model.score(X_test_transformed, y_test).round(2)}')\r\n",
    "# Feature Importances\r\n",
    "feature_imp_rf = pd.Series(random_forest_model.feature_importances_, index=store_demographic_columns).sort_values()\r\n",
    "print('Top 3 features:', list(feature_imp_rf[-3:].index)[::-1])\r\n",
    "# Plot\r\n",
    "feature_imp_rf.plot(kind='barh', title='Random Forest: Feature Importances')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot Feature Importance: Gradient Boosting\r\n",
    "gradient_boosting_model = GradientBoostingClassifier().fit(X_train_transformed, y_train)\r\n",
    "print(f'Gradient Boosting Score in Validation Data: {gradient_boosting_model.score(X_test_transformed, y_test).round(2)}')\r\n",
    "# Feature Importances\r\n",
    "feature_imp_gb = pd.Series(gradient_boosting_model.feature_importances_, index=store_demographic_columns).sort_values()\r\n",
    "print('Top 3 features:', list(feature_imp_gb[-3:].index)[::-1])\r\n",
    "# Plot\r\n",
    "feature_imp_gb.plot(kind='barh', title='Gradient Boosting: Feature Importances')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Function: To predict New Store Segment\r\n",
    "# Input Data\r\n",
    "new_data = store_demographic_with_info_data_new\r\n",
    "transformed_data = column_transformer.transform(new_data)\r\n",
    "\r\n",
    "def predict_new_store_segment(model, new_store_data=new_data, transformered_data=transformed_data):\r\n",
    "    pred = model.predict(transformed_data)\r\n",
    "    temp_df = new_store_data.assign(Segment = pred)\r\n",
    "    \r\n",
    "    return temp_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Predict Segment for New Store\r\n",
    "segment_result_dt = predict_new_store_segment(decision_tree_model)\r\n",
    "segment_result_rf = predict_new_store_segment(random_forest_model)\r\n",
    "segment_result_gb = predict_new_store_segment(gradient_boosting_model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# merge the result for visualazation\r\n",
    "columns_merge = ['Store', 'Segment']\r\n",
    "temp_merge_df = segment_result_dt[['Store', 'Type', 'Segment']].merge(segment_result_rf[columns_merge], on='Store', suffixes=('_Tree', '_Forest'))\r\n",
    "temp_merge_df.merge(segment_result_gb[columns_merge], on='Store').rename(columns={'Segment': 'Segment_Boost'})\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "columns_store_form_report = ['Store', 'Segment']\r\n",
    "print('\\nThe Segment for New Stores')\r\n",
    "segment_result_gb[columns_store_form_report]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 3: Forecasting\r\n",
    "Fresh produce has a short life span, and due to increasing costs, the company wants to have an accurate monthly sales forecast.\r\n",
    "\r\n",
    "You’ve been asked to prepare a monthly forecast for produce sales for the full year of 2016 for both existing and new stores.\r\n",
    "\r\n",
    "Note: Use a 6 month holdout sample for the TS Compare tool (this is because we do not have that much data so using a 12 month holdout would remove too much of the data)\r\n",
    "\r\n",
    "## Task 3 Submission\r\n",
    "1. What type of ETS or ARIMA model did you use for each forecast? Use ETS(a,m,n) or ARIMA(ar, i, ma) notation. How did you come to that decision?\r\n",
    "\r\n",
    "\r\n",
    "2. Please provide a table of your forecasts for existing and new stores. Also, provide visualization of your forecasts that includes historical data, existing stores forecasts, and new stores forecasts.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Store ans segement pairs\r\n",
    "# Variables to work with\r\n",
    "columns_store_segment = ['Store', 'Segment']\r\n",
    "# existing stores\r\n",
    "existing_store_segment = filtered_stores_sales_2015_data[columns_store_segment]\r\n",
    "# new stores\r\n",
    "new_store_segement = segment_result_gb[columns_store_segment]\r\n",
    "# Join Store Sales with Segment\r\n",
    "stores_sales_with_segment_data = stores_sales_data.merge(existing_store_segment, on='Store')\r\n",
    "stores_sales_with_segment_data.head(3)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Aggregate Monthly Produce sales for existing store forecast\r\n",
    "# existing_store_monthly_sales_data \r\n",
    "tmp_df= stores_sales_with_segment_data.groupby(['Year', 'Month'], as_index=False)['Produce'].agg({'Monthly_Sales': 'sum'})\r\n",
    "# convert Year and Month columns to datetimeindex\r\n",
    "tmp_date = tmp_df['Year'].astype(str) + '-' + tmp_df['Month'].astype(str)\r\n",
    "tmp_df = tmp_df.assign(Date = pd.to_datetime(tmp_date))\r\n",
    "existing_store_monthly_sales_data = tmp_df.set_index('Date', drop=True)['Monthly_Sales']\r\n",
    "existing_store_monthly_sales_data.index.freq ='MS'\r\n",
    "print('\\nMonthly Produce Sales Data for Existing Stores')\r\n",
    "existing_store_monthly_sales_data.head(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Aggregate Monthly Produce sales for new store forecast \r\n",
    "tmp_df = stores_sales_with_segment_data.groupby(['Store','Year', 'Month', 'Segment'], as_index=False)['Produce'].agg({'Monthly_Sales': 'sum'})\r\n",
    "tmp_df = tmp_df.groupby(['Year', 'Month', 'Segment'], as_index=False)['Monthly_Sales'].agg({'Avg_Monthly_Sales': 'mean'})\r\n",
    "# convert Year and Month columns to datetimeindex\r\n",
    "tmp_date = tmp_df['Year'].astype(str) + '-' + tmp_df['Month'].astype(str)\r\n",
    "segment_store_monthly_sales_data = tmp_df.assign(Date = pd.to_datetime(tmp_date))\r\n",
    "# Sales per Segment\r\n",
    "# Segment 0\r\n",
    "segment_0_store_monthly_sales_data = segment_store_monthly_sales_data.query('Segment == 0').set_index('Date', drop=True)['Avg_Monthly_Sales']\r\n",
    "segment_0_store_monthly_sales_data.index.freq ='MS'\r\n",
    "print('\\nMonthly Produce Sales Data for segment 0 Stores')\r\n",
    "print(segment_0_store_monthly_sales_data.head(2))\r\n",
    "# Segment 1\r\n",
    "segment_1_store_monthly_sales_data = segment_store_monthly_sales_data.query('Segment == 1').set_index('Date', drop=True)['Avg_Monthly_Sales']\r\n",
    "segment_1_store_monthly_sales_data.index.freq ='MS'\r\n",
    "print('\\nMonthly Produce Sales Data for segment 1 Stores')\r\n",
    "print(segment_1_store_monthly_sales_data.head(2))\r\n",
    "# Segment 2\r\n",
    "segment_2_store_monthly_sales_data = segment_store_monthly_sales_data.query('Segment == 2').set_index('Date', drop=True)['Avg_Monthly_Sales']\r\n",
    "segment_2_store_monthly_sales_data.index.freq ='MS'\r\n",
    "print('\\nMonthly Produce Sales Data for segment 2 Stores')\r\n",
    "print(segment_2_store_monthly_sales_data.head(2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot Aggregated Monthly Produce Sales\r\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 9))\r\n",
    "axs[0].plot(existing_store_monthly_sales_data, label='Existing Stores')\r\n",
    "axs[0].set_title('Monthly Produce Sales for Existing Stores')\r\n",
    "axs[0].legend()\r\n",
    "\r\n",
    "axs[1].plot(segment_0_store_monthly_sales_data, label='Segment 0 Stores')\r\n",
    "axs[1].plot(segment_1_store_monthly_sales_data, label='Segment 1 Stores')\r\n",
    "axs[1].plot(segment_2_store_monthly_sales_data, label='Segment 2 Stores')\r\n",
    "axs[1].set_title('Monthly Produce Sales for Segment')\r\n",
    "axs[1].legend()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Monthly Produce Sales and Segment Sales follow similar patten  \r\n",
    "Let find best model for Mothly sales then use it to train for segment sales"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Time Series Train Test Data\r\n",
    "def time_series_train_test_split(df_with_date_index_data, holdout_size=6):\r\n",
    "    train = df_with_date_index_data.iloc[:-holdout_size]\r\n",
    "    test = df_with_date_index_data.iloc[-holdout_size:]\r\n",
    "    return train, test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Grid Search ETS Model\r\n",
    "# Search Parameters: error, trend, damped_trend, seasonal, seasonal_periods\r\n",
    "\r\n",
    "# helper: Format ETS to return single capital letter\r\n",
    "def helper_ets_format(value):\r\n",
    "    if not value:\r\n",
    "        return 'N'\r\n",
    "    return value[0].upper()\r\n",
    "\r\n",
    "# intial values for Error, Trend and Seasonal\r\n",
    "e = ['add', 'mul']\r\n",
    "t = ['add', 'mul']\r\n",
    "s = ['add', 'mul', None]\r\n",
    "\r\n",
    "def ets_grid_search(train, test, errors=e, trends=t, is_damped=[True, False], seasonals=s):\r\n",
    "    tmp_result = []\r\n",
    "    for error in errors:\r\n",
    "        for damped in is_damped:\r\n",
    "            if not damped:\r\n",
    "                trends = ['add', 'mul', None]\r\n",
    "            for trend in trends:\r\n",
    "                for seasonal in seasonals:\r\n",
    "                    ets_model = ETSModel(train, error=error, trend=trend, damped_trend=damped, seasonal=seasonal, seasonal_periods=12)\r\n",
    "                    ets_model_fit = ets_model.fit(disp=0)\r\n",
    "                    rmse_result = rmse(test, ets_model_fit.forecast(6)).round()\r\n",
    "                    ets_value = f'ETS{helper_ets_format(error), helper_ets_format(trend), helper_ets_format(seasonal)}'\r\n",
    "                    tmp_parameters = [ets_value, damped, rmse_result]\r\n",
    "                    tmp_result.append(tmp_parameters)\r\n",
    "            trends = ['add', 'mul']\r\n",
    "    tmp_df = pd.DataFrame(tmp_result, columns=['Model', 'Damped_Trend', 'RMSE'])\r\n",
    "    tmp_df = tmp_df.sort_values(by='RMSE')\r\n",
    "    return tmp_df.assign(RMSE = lambda x: x.RMSE.map('{:,}'.format))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Run ETS Grid Search\r\n",
    "train, test = time_series_train_test_split(existing_store_monthly_sales_data)\r\n",
    "# est_gridsearch_result = ets_grid_search(train, test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Best ETS Model\r\n",
    "print('ETS Top 5 Best Model')\r\n",
    "# est_gridsearch_result.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Grid Search ARIMA Model\r\n",
    "# Search Parameters: order, trend\r\n",
    "possible_p = range(1,13)\r\n",
    "possible_q = range(13)\r\n",
    "possible_d = range(3) #use pmdarima ndiff function to finds max d value for adf, kpss and pp test we get 2\r\n",
    "possible_trend = ['n', 'c', 't', 'ct']\r\n",
    "\r\n",
    "def arima_grid_search(train, test, ps=possible_p, ds=possible_d, qs=possible_q, trends=possible_trend):\r\n",
    "    tmp_result = []\r\n",
    "    for d in ds:\r\n",
    "        for p in ps:\r\n",
    "            for q in qs:\r\n",
    "                for trend in trends:\r\n",
    "                    try:\r\n",
    "                        model = ARIMA(train, order=(p,d,q), trend=trend)\r\n",
    "                        model_fit = model.fit()\r\n",
    "                        rmse_result = rmse(test, model_fit.forecast(6)).round()\r\n",
    "                    except:\r\n",
    "                        rmse_result = float('inf')\r\n",
    "\r\n",
    "                    arima_value = f'ARIMA{p,d,q}'\r\n",
    "                    tmp_parameters = [arima_value, trend, rmse_result]\r\n",
    "                    tmp_result.append(tmp_parameters)\r\n",
    "\r\n",
    "    tmp_df = pd.DataFrame(tmp_result, columns=['Model', 'Trend', 'RMSE'])\r\n",
    "    tmp_df = tmp_df.sort_values(by='RMSE')\r\n",
    "    return tmp_df.assign(RMSE = lambda x: x.RMSE.map('{:,}'.format))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Run ARIMA Grid Search\r\n",
    "train, test = time_series_train_test_split(existing_store_monthly_sales_data)\r\n",
    "arima_gridsearch_result = arima_grid_search(train, test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Best ARIMA Model\r\n",
    "print('ARIMA Top 5 Best Model')\r\n",
    "arima_gridsearch_result.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Model Selection:\n",
    "# ETS Model: ETS(A,M,M) with damped trend\n",
    "ets_model = ETSModel(train, error='add', trend='mul', damped_trend=True, seasonal='mul', seasonal_periods=12)\n",
    "ets_model_fit = ets_model.fit(disp=0)\n",
    "ets_forecast = ets_model_fit.forecast(6).round(2)\n",
    "\n",
    "# ARIMA Model: ARIMA(0,0,2) with constant trend 'ct'\n",
    "arima_model = ARIMA(train, order=(11,0,7), trend='c')\n",
    "arima_model_fit = arima_model.fit()\n",
    "arima_forecast = arima_model_fit.forecast(6).round(2)\n",
    "\n",
    "# Plot Models Holdout Forecast\n",
    "fig, axs = plt.subplots(1, 1, figsize=(12, 6))\n",
    "# Plot Actual Sales\n",
    "axs.plot(existing_store_monthly_sales_data, label='Actual Sales')\n",
    "# Plot ETS forecast Sales\n",
    "axs.plot(ets_forecast, label='ETS(A,M,M) Forecast')\n",
    "# Plot ARIMA forecast Sales\n",
    "axs.plot(arima_forecast, label='ARIMA(11,0,7) Forecast')\n",
    "# add title and legend\n",
    "axs.set_title('Monthly Produce Sales for Existing Stores')\n",
    "axs.legend()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Forecast 2016 Sales for Existing Stores\n",
    "# Train the model with all data\n",
    "existing_stores_forecast = ARIMA(existing_store_monthly_sales_data, order=(11,0,7), trend='c').fit().forecast(12).round()\n",
    "existing_stores_forecast"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Forecast 2016 Sales for New Stores\r\n",
    "# Train the model with all data in segment\r\n",
    "segment_0_stores_forecast = ARIMA(segment_0_store_monthly_sales_data, order=(11,0,7), trend='c').fit().forecast(12).round()\r\n",
    "segment_1_stores_forecast = ARIMA(segment_1_store_monthly_sales_data, order=(11,0,7), trend='c').fit().forecast(12).round()\r\n",
    "segment_2_stores_forecast = ARIMA(segment_2_store_monthly_sales_data, order=(11,0,7), trend='c').fit().forecast(12).round()\r\n",
    "# commbine segments\r\n",
    "segment_stores_forecast = pd.concat([segment_0_stores_forecast, segment_1_stores_forecast, segment_2_stores_forecast], axis=1)\r\n",
    "segment_stores_forecast.columns = ['Segment_0', 'Segment_1', 'Segment_2']\r\n",
    "# Count the segment in best cluster selected\r\n",
    "best_cluster = segment_result_gb\r\n",
    "segment_counts = best_cluster['Segment'].value_counts()\r\n",
    "# add total sales per month\r\n",
    "segment_stores_forecast = segment_stores_forecast.assign(Monthly_Sales = lambda x: x.Segment_0*segment_counts[0]+x.Segment_1*segment_counts[1]+x.Segment_2*segment_counts[2])\r\n",
    "segment_stores_forecast['Monthly_Sales']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# combine forecasted data\r\n",
    "forecasted_result = pd.concat([existing_stores_forecast, segment_stores_forecast['Monthly_Sales']], axis=1)\r\n",
    "forecasted_result.columns = ['Existing_Stores_Forecast', 'New_Stores_Forecast']\r\n",
    "forecasted_result.index = forecasted_result.index.to_period('M')\r\n",
    "# total mothly forecast sales\r\n",
    "forecasted_result = forecasted_result.assign(Total_Stores_Forecast = lambda x: x.Existing_Stores_Forecast + x.New_Stores_Forecast)\r\n",
    "forecasted_result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot Models 2016 Forecast\r\n",
    "fig, axs = plt.subplots(1, 1, figsize=(12, 7))\r\n",
    "axs.plot(existing_store_monthly_sales_data, label='Historical data')\r\n",
    "axs.plot(forecasted_result['Existing_Stores_Forecast'], label='Forecast Existing Stores')\r\n",
    "axs.plot(forecasted_result['Total_Stores_Forecast'], label='Forecast with New Stores')\r\n",
    "axs.set_title('Forecast 2016 Monthly Produce Sales')\r\n",
    "axs.legend()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "5cba3963c37d400916a9e80ad5acd0c56daed180f4961ad2a064f19bb9bbda97"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}