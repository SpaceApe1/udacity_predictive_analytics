{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Combining Predictive Techniques"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Given\r\n",
    "\r\n",
    "* StoreSalesData.csv - This file contains sales by product category for all existing stores for 2012, 2013, and 2014.\r\n",
    "* StoreInformation.csv - This file contains location data for each of the stores.\r\n",
    "* StoreDemographicData.csv - This file contains demographic data for the areas surrounding each of the existing stores and locations for new stores."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load Package"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import calinski_harabasz_score, silhouette_score, davies_bouldin_score, plot_roc_curve\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.exponential_smoothing.ets import ETSModel\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tools.eval_measures import rmse, rmspe\n",
    "from statsmodels.graphics import tsaplots\n",
    "import pmdarima as pm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [11, 7]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load Stores Sales\n",
    "stores_sales_data = pd.read_csv('storesalesdata.csv')\n",
    "# Bad Data: There no date 29-Feb-2014, Drod the data\n",
    "# stores_sales_data = stores_sales_data.query('Date != \"2014 02 29\"')\n",
    "# Convert Date varible to daterime object\n",
    "# stores_sales_data = stores_sales_data.assign(Date = pd.to_datetime(stores_sales_data['Date']))\n",
    "\n",
    "stores_sales_data.head(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load Store Information\n",
    "store_information_data = pd.read_csv('storeinformation.csv')\n",
    "store_information_data.head(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load Store Demographic Data\n",
    "store_demographic_data = pd.read_csv('storedemographicdata.csv')\n",
    "store_demographic_data.head(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 1: Store Format (segments) for Existing Stores\r\n",
    "\r\n",
    "To remedy the product surplus and shortages, the company wants to introduce different store formats. Each store format will have a different product selection in order to better match local demand. The actual building sizes will not change, just the product selection and internal layouts.\r\n",
    "\r\n",
    "* Determine the optimal number of store formats based on sales data.\r\n",
    "    - Sum sales data by StoreID and Year\r\n",
    "    - Use percentage sales per category per store for clustering (category sales as a percentage of total store sales).\r\n",
    "    - Use only 2015 sales data.\r\n",
    "    - Use a K-means clustering model.\r\n",
    "\r\n",
    "* Segment the 85 current stores into the different store formats.\r\n",
    "* Use the StoreSalesData.csv and StoreInformation.csv files.\r\n",
    "\r\n",
    "## Task 1 Submission\r\n",
    "1. What is the optimal number of store formats? How did you arrive at that number?\r\n",
    "2. How many stores fall into each store format?\r\n",
    "3. Based on the results of the clustering model, what is one way that the clusters differ from one another?\r\n",
    "4. Please provide a map created in Tableau that shows the location of the existing stores, uses color to show cluster, and size to show total sales. Make sure to include a legend! Feel free to simply copy and paste the map into the submission template."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Aggregate sum of sales by Store and Year\n",
    "filtered_columns = ['Dry_Grocery', 'Dairy', 'Frozen_Food', 'Meat', 'Produce', 'Floral', 'Deli', 'Bakery', 'General_Merchandise']\n",
    "filtered_stores_data =  stores_sales_data.groupby(['Store', 'Year'], as_index=False)[filtered_columns].sum()\n",
    "# Add Store Total Sales per year\n",
    "filtered_stores_data = filtered_stores_data.assign(Total_Sales = filtered_stores_data[filtered_columns].sum(axis=1))\n",
    "# Calculate percentage sales per category per store\n",
    "filtered_stores_data[filtered_columns] = filtered_stores_data[filtered_columns].div(filtered_stores_data['Total_Sales'], axis=0)\n",
    "\n",
    "# Filter 2015 data\n",
    "filtered_stores_sales_2015_data = filtered_stores_data.query('Year == 2015')\n",
    "\n",
    "print('\\nFiltered and Aggregated 2015 Store Data')\n",
    "filtered_stores_sales_2015_data.head()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Find best numbers of clusters\n",
    "\n",
    "# Function to return pandas describe\n",
    "def cluster_number_test(raw_data, score_metric, number_test, cluster_number_list):\n",
    "    df_list = []\n",
    "    scaler = MinMaxScaler().fit_transform(raw_data)\n",
    "\n",
    "    for i in cluster_number_list:\n",
    "        score_list = []\n",
    "        for _ in range(number_test):\n",
    "            kmeans = KMeans(n_clusters=i)\n",
    "            kmeans.fit(scaler)\n",
    "            pred = kmeans.predict(scaler)\n",
    "            score = score_metric(raw_data, pred)\n",
    "            score_list.append(score)\n",
    "\n",
    "        temp_df = pd.DataFrame(score_list)\n",
    "        df_list.append(temp_df)\n",
    "\n",
    "    column_names = [f'Cluster {i}' for i in cluster_number_list]\n",
    "    _df = pd.concat(df_list, axis=1)\n",
    "    _df.columns = column_names\n",
    "    return _df.describe().round(2)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Finding best number of cluster\n",
    "raw_data = filtered_stores_sales_2015_data[filtered_columns]\n",
    "test_count = 100\n",
    "possible_clusters = range(2,8)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Run Test\n",
    "c_h_score = cluster_number_test(raw_data, calinski_harabasz_score, test_count, possible_clusters)\n",
    "sil_score = cluster_number_test(raw_data, silhouette_score, test_count, possible_clusters)\n",
    "d_b_score = cluster_number_test(raw_data, davies_bouldin_score, test_count, possible_clusters)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Metric: Calinski Harabasz Score - Higher the better\n",
    "print(f'Run Calinski Harabasz Score Test {test_count} times - Higher the better')\n",
    "print(c_h_score, '\\n')\n",
    "# Metric: Silhoutte Score - Higher the better\n",
    "print(f'Run Silhoutte Score Score Test {test_count} times - Higher the better')\n",
    "print(sil_score, '\\n')\n",
    "# Metric: Davies Bouldin Score - Smaller the better\n",
    "print(f'Run Davies Bouldin Score Test {test_count} times - Smaller the better')\n",
    "print(d_b_score)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# From above test, what is the best number of cluster?\n",
    "\n",
    "##### Score Results #####\n",
    "# Calinski Harabasz Score: 2 Cluster\n",
    "# Silhoutte Score: 3 Cluster\n",
    "# Davies Bouldin Score: 3 Cluster\n",
    "\n",
    "best_number_cluster = 3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Clusters - number of clusters = 3 \n",
    "kmeans = KMeans(n_clusters = best_number_cluster)\n",
    "# scale data\n",
    "scaled_data = MinMaxScaler().fit_transform(raw_data)\n",
    "kmeans.fit(scaled_data)\n",
    "# Add cluser laber to data\n",
    "pred = kmeans.predict(scaled_data)\n",
    "filtered_stores_sales_2015_data = filtered_stores_sales_2015_data.assign(Segment = pred)\n",
    "\n",
    "# check numbers of stores in each Segment\n",
    "print('Number of stores in Segment')\n",
    "print(filtered_stores_sales_2015_data['Segment'].value_counts())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Merge filtered store sale with store\n",
    "filtered_stores_sales_2015_merged_information_data = filtered_stores_sales_2015_data.merge(store_information_data, how='inner', on='Store')\n",
    "filtered_stores_sales_2015_merged_information_data.head(2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot Clusters\n",
    "# Scatter Markers Customizing: Size, Color, Shape, Transparency\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 2: Store Format for New Stores\r\n",
    "\r\n",
    "The grocery store chain has 10 new stores opening up at the beginning of the year. The company wants to determine which store format each of the new stores should have. However, we don’t have sales data for these new stores yet, so we’ll have to determine the format using each of the new store’s demographic data.\r\n",
    "\r\n",
    "You’ve been asked to:\r\n",
    "\r\n",
    "* Develop a model that predicts which segment a store falls into based on the demographic and socioeconomic characteristics of the population that resides in the area around each new store.\r\n",
    "* Use a 20% validation sample with Random Seed = 3 when creating samples with which to compare the accuracy of the models. Make sure to compare a decision tree, forest, and boosted model.\r\n",
    "* Use the model to predict the best store format for each of the 10 new stores.\r\n",
    "* Use the StoreDemographicData.csv file, which contains the information for the area around each store.\r\n",
    "\r\n",
    "Note: In a real world scenario, you could use PCA to reduce the number of predictor variables. However, there is no need to do so in this project. You can leave all predictor variables in the model.\r\n",
    "\r\n",
    "\r\n",
    "## Task 2 Submission\r\n",
    "* What methodology did you use to predict the best store format for the new stores? Why did you choose that methodology?\r\n",
    "* What are the three most important variables that help explain the relationship between demographic indicators and store formats? Please include a visualization.\r\n",
    "* What format do each of the 10 new stores fall into? Please provide a data table.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 44 variables in demographic store data\n",
    "store_demographic_columns = ['Age0to9', 'Age10to17', 'Age18to24', 'Age25to29', 'Age30to39', 'Age40to49', 'Age50to64', 'Age65Plus', 'EdLTHS', 'EdHSGrad', 'EdSomeCol', 'EdAssociate', 'EdBachelor', 'EdMaster', 'EdProfSchl', 'EdDoctorate', 'HHSz1Per', 'HHSz2Per', 'HHSz3Per', 'HHSz4Per', 'HHSz5PlusPer', 'HHIncU25K', 'HHInc25Kto50K', 'HHInc50Kto75K',\n",
    "       'HHInc75Kto100K', 'HHInc100Kto150K', 'HHInc150Kto250K', 'HHInc250KPlus', 'PopAsian', 'PopBlack', 'PopHispanic', 'PopMulti', 'PopNativeAmer', 'PopOther', 'PopPacIsl', 'PopWhite', 'HVal0to100K', 'HVal100Kto200K', 'HVal200Kto300K', 'HVal300Kto400K', 'HVal400Kto500K', 'HVal500Kto750K', 'HVal750KPlus', 'PopDens']\n",
    "\n",
    "# join demographic data with store information\n",
    "store_info_columns = ['Store', 'Type']\n",
    "store_demographic_with_info_data = store_demographic_data.merge(store_information_data[store_info_columns], on='Store')\n",
    "\n",
    "#  filter existing store\n",
    "store_demographic_with_info_data_existing = store_demographic_with_info_data.query('Type == \"Existing\"')\n",
    "#  filter new store\n",
    "store_demographic_with_info_data_new = store_demographic_with_info_data.query('Type == \"New\"')\n",
    "\n",
    "# merge segment to existing store\n",
    "store_filtered_columns = ['Store', 'Segment']\n",
    "_temp_df = filtered_stores_sales_2015_merged_information_data[store_filtered_columns]\n",
    "store_demographic_with_info_data_existing = store_demographic_with_info_data_existing.merge(_temp_df, on='Store')\n",
    "\n",
    "# Prepair X and y for training\n",
    "y = store_demographic_with_info_data_existing['Segment']\n",
    "X = store_demographic_with_info_data_existing.drop(columns='Segment')\n",
    "# Split train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=3)\n",
    "# Column Transformer\n",
    "column_transformer = ColumnTransformer([('numerical', MinMaxScaler(), store_demographic_columns)])\n",
    "column_transformer.fit(X_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Classifier Score on N run test\n",
    "# Transform Train Data\n",
    "X_train_transformed = column_transformer.transform(X_train)\n",
    "# Transform Test Data\n",
    "X_test_transformed = column_transformer.transform(X_test)\n",
    "\n",
    "def classifier_test_score(estimator, X_train=X_train_transformed, y_train=y_train, X_test=X_test_transformed, y_test=y_test, cv=100):\n",
    "    _score_list = []\n",
    "    for _ in range(cv):\n",
    "        _cls = estimator().fit(X_train, y_train)\n",
    "        _score = _cls.score(X_test, y_test)\n",
    "        _score_list.append(_score)\n",
    "    return np.array(_score_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Decision Tree Classifier \n",
    "decision_tree_result = classifier_test_score(DecisionTreeClassifier)\n",
    "# Random Forest Classifier\n",
    "random_forest_result = classifier_test_score(RandomForestClassifier)\n",
    "# Gradient Boosting Classifier\n",
    "gradient_boosting = classifier_test_score(GradientBoostingClassifier)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Model Score\n",
    "print('Model Score in Validation Data')\n",
    "print(f'Decision Tree Average Score in Validation Data: {decision_tree_result.mean().round(2)}')\n",
    "print(f'Random Forest Average Score in Validation Data: {random_forest_result.mean().round(2)}')\n",
    "print(f'Gradient Boosting Average Score in Validation Data: {gradient_boosting.mean().round(2)}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot Feature Importance: Decision Tree\n",
    "# Score\n",
    "decision_tree_model = DecisionTreeClassifier().fit(X_train_transformed, y_train)\n",
    "print(f'Decision Tree Score in Validation Data: {decision_tree_model.score(X_test_transformed, y_test).round(2)}')\n",
    "# Plot\n",
    "feature_imp_dt = decision_tree_model.feature_importances_\n",
    "pd.Series(feature_imp_dt, index=store_demographic_columns).sort_values().plot(kind='barh', title='Decision Tree: Feature Importances')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot Feature Importance: Random Forest\n",
    "# Score\n",
    "random_forest_model = RandomForestClassifier().fit(X_train_transformed, y_train)\n",
    "print(f'Decision Tree Score in Validation Data: {random_forest_model.score(X_test_transformed, y_test).round(2)}')\n",
    "# Plot\n",
    "feature_imp_rf = random_forest_model.feature_importances_\n",
    "pd.Series(feature_imp_rf, index=store_demographic_columns).sort_values().plot(kind='barh', title='Random Forest: Feature Importances')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot Feature Importance: Gradient Boosting\n",
    "gradient_boosting_model = GradientBoostingClassifier().fit(X_train_transformed, y_train)\n",
    "print(f'Decision Tree Score in Validation Data: {gradient_boosting_model.score(X_test_transformed, y_test).round(2)}')\n",
    "# Plot\n",
    "feature_imp_gb = gradient_boosting_model.feature_importances_\n",
    "pd.Series(feature_imp_gb, index=store_demographic_columns).sort_values().plot(kind='barh', title='Gradient Boosting: Feature Importances')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Function: To predict New Store Segment\n",
    "# Input Data\n",
    "new_data = store_demographic_with_info_data_new\n",
    "transformed_data = column_transformer.transform(new_data)\n",
    "\n",
    "def predict_new_store_segment(model, new_store_data=new_data, transformered_data=transformed_data):\n",
    "    pred = model.predict(transformed_data)\n",
    "    temp_df = new_store_data.assign(Segment = pred)\n",
    "    \n",
    "    return temp_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Predict Segment for New Store\n",
    "segment_result_dt = predict_new_store_segment(decision_tree_model)\n",
    "segment_result_rf = predict_new_store_segment(random_forest_model)\n",
    "segment_result_gb = predict_new_store_segment(gradient_boosting_model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# merge the result for visualazation\n",
    "columns_merge = ['Store', 'Segment']\n",
    "temp_merge_df = segment_result_dt[['Store', 'Type', 'Segment']].merge(segment_result_rf[columns_merge], on='Store', suffixes=('_Tree', '_Forest'))\n",
    "temp_merge_df.merge(segment_result_gb[columns_merge], on='Store').rename(columns={'Segment': 'Segment_Boost'})\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 3: Forecasting\r\n",
    "Fresh produce has a short life span, and due to increasing costs, the company wants to have an accurate monthly sales forecast.\r\n",
    "\r\n",
    "You’ve been asked to prepare a monthly forecast for produce sales for the full year of 2016 for both existing and new stores.\r\n",
    "\r\n",
    "Note: Use a 6 month holdout sample for the TS Compare tool (this is because we do not have that much data so using a 12 month holdout would remove too much of the data)\r\n",
    "\r\n",
    "## Task 3 Submission\r\n",
    "1. What type of ETS or ARIMA model did you use for each forecast? Use ETS(a,m,n) or ARIMA(ar, i, ma) notation. How did you come to that decision?\r\n",
    "\r\n",
    "\r\n",
    "2. Please provide a table of your forecasts for existing and new stores. Also, provide visualization of your forecasts that includes historical data, existing stores forecasts, and new stores forecasts.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Prepare Data for Forecasting\n",
    "# Agregate MonthlyData\n",
    "temp_agg_df = stores_sales_data.groupby(['Year', 'Month'], as_index=False)['Produce'].sum()\n",
    "# Create DateTime\n",
    "temp_date = temp_agg_df['Year'].astype(str) + '-' + temp_agg_df['Month'].astype(str)\n",
    "# Agg Results\n",
    "temp_agg_df = temp_agg_df.assign(Date = pd.to_datetime(temp_date))\n",
    "produce_monthly_sales = temp_agg_df.set_index('Date')\n",
    "produce_monthly_sales.index.freq ='MS'\n",
    "# Validation Data\n",
    "produce_monthly_sales_test_data = produce_monthly_sales.query('Year == 2015 and Month > 6')\n",
    "# Test Data\n",
    "temp_condition = produce_monthly_sales.index.isin(produce_monthly_sales_test_data.index)\n",
    "produce_monthly_sales_train_data = produce_monthly_sales[~temp_condition]\n",
    "\n",
    "produce_monthly_sales_train_data.head(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Study how ETS works in statsmodels\n",
    "* First: Plot Data \n",
    "* second: Decompose Data and plot Trend, Seasonality ans Residual \n",
    "* Third: Choose ETS type (A,M, or N) "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot TS Data\n",
    "produce_monthly_sales.plot(y='Produce', title='Produce Monthly Sales')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Decompose Data and plot Trend, Seasonality ans Residual\n",
    "# model can be add or mul\n",
    "decompose_result = seasonal_decompose(produce_monthly_sales['Produce'], model='add', period=12, extrapolate_trend='freq')\n",
    "decompose_result.plot()\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Grid Search ETS Model Parameter\n",
    "\n",
    "error_value = ['add', 'mul']\n",
    "trends_value = ['add', 'mul', None]\n",
    "\n",
    "def ets_model_grid_search(train, test, is_damped=False, errors=error_value, trends=trends_value, seasonals=trends_value, period=12): \n",
    "    temp_result = []\n",
    "    columns_name = ['ETS Model', 'RMSE', 'RMSPE']\n",
    "\n",
    "    for error_value in errors:\n",
    "        for trend_value in trends:\n",
    "            for seasonal_value in seasonals:\n",
    "                model = ETSModel(train, error=error_value, trend=trend_value, damped_trend=is_damped, seasonal=seasonal_value, seasonal_periods=period)\n",
    "                model_fit = model.fit()\n",
    "                forecast = model_fit.forecast(6).round(2)\n",
    "                temp_list = []\n",
    "                model_type = f'ETS{error_value, trend_value, seasonal_value}'\n",
    "                temp_list.append(model_type)\n",
    "                temp_list.append(rmse(test, forecast))\n",
    "                temp_list.append(rmspe(test, forecast))\n",
    "                temp_result.append(temp_list)\n",
    "\n",
    "    return pd.DataFrame(temp_result, columns=columns_name).sort_values(by=['RMSE', 'RMSPE']).round(2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Search for the best Model Parameter\n",
    "train_data = produce_monthly_sales_train_data['Produce']\n",
    "test_data = produce_monthly_sales_test_data['Produce']\n",
    "\n",
    "res_non_damped_trend = ets_model_grid_search(train_data, test_data)\n",
    "res_damped_trend = ets_model_grid_search(train_data, test_data, trends=['add', 'mul'], is_damped=True)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check Results\n",
    "print('The Top 5 Best Models with No Trend Damp')\n",
    "print(res_non_damped_trend.iloc[:5])\n",
    "\n",
    "print('\\nThe Top 5 Best Models with Trend Damp')\n",
    "print(res_damped_trend.iloc[:5])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Choose ETS type (A,M, or N) \n",
    "# Form decompose plot: ETS(M,A,A)\n",
    "# From Gridsearch: ETS(A,M,M) with trend dump\n",
    "# From Gridsearch: ETS(A,A,M) with no trend dump\n",
    "\n",
    "# train selected model with full data and forecast next 12 months\n",
    "ets_model = ETSModel(produce_monthly_sales['Produce'], error='add', trend='mul', seasonal='mul', seasonal_periods=12)\n",
    "ets_model_fit = ets_model.fit()\n",
    "# Validate Forecast on hold out sample\n",
    "forecast_result = ets_model_fit.forecast(12).round(2)\n",
    "\n",
    "# Forecasted Results\n",
    "forecast_result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Study how ARIMA works in statsmodels\n",
    "\n",
    "ARIMA stand for Auto Regressive Integrated Moving Average \n",
    "\n",
    "Order:\n",
    " - p - The order of AR term\n",
    " - d - The number of differencing required to make the time series stationary\n",
    " - q - The oder of MA term\n",
    "\n",
    "Use: pmdarima package"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# plot acf and pacf\n",
    "fig, axs = plt.subplots(2, 1)\n",
    "\n",
    "tsaplots.plot_acf(train_data, alpha=.05, ax=axs[0])\n",
    "# count signicant lag: 2, this will be max range of q\n",
    "\n",
    "tsaplots.plot_pacf(train_data, alpha=.05, ax=axs[1])\n",
    "# count signicant lag: 3, this will be max range of p\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Test Stationary: Find d value\n",
    "n_adf = pm.arima.ndiffs(train_data, alpha=0.05, test='adf', max_d = 12)\n",
    "n_kpss = pm.arima.ndiffs(train_data, alpha=0.05, test='kpss', max_d = 12)\n",
    "n_pp = pm.arima.ndiffs(train_data, alpha=0.05, test='pp', max_d = 12)\n",
    "\n",
    "d = max(n_adf, n_kpss, n_pp)\n",
    "\n",
    "print(f'n_adf = {n_adf}, n_kpss = {n_kpss}, n_pp = {n_pp} : d -> {d}')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}